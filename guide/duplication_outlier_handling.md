# üîç Complete Duplication and Outlier Handling Guide

This guide is ideal for data analysts, data scientists, and ML engineers working in Python who want a systematic, step-by-step approach to handle duplication and outliers in their datasets.

This guide is a part of a larger series on Machine Learning Pipelines. Each section is designed to be modular, allowing you to adapt the code and techniques to your specific dataset and analysis needs.

**The guide before:** [Missing Data Imputation](./missing_data_imputation.md)  
**The guide after:** [Categorical Variable Cleaning](./categorical_variable_cleaning.md)

## üìã Table of Contents

1. [üìñ Overview](#-overview)
2. [üîÑ Understanding Duplicates](#-understanding-duplicates)
3. [üéØ Duplicate Detection and Handling](#-duplicate-detection-and-handling)
4. [üìä Understanding Outliers](#-understanding-outliers)
5. [üîç Outlier Detection Methods](#-outlier-detection-methods)
6. [üõ†Ô∏è Outlier Handling Strategies](#Ô∏è-outlier-handling-strategies)
7. [üíª Code Examples and Implementation](#-code-examples-and-implementation)
8. [‚úÖ Validation and Quality Checks](#-validation-and-quality-checks)
9. [‚≠ê Best Practices](#-best-practices)
10. [‚ö†Ô∏è Common Pitfalls](#Ô∏è-common-pitfalls)
11. [üìù Summary](#-summary)

## üìñ Overview

Just like missing data, duplication and outliers can significantly impact the performance of machine learning models. Your EDA should include a thorough examination of your dataset for duplicates and outliers, as well as strategies to handle them. So again EDA is a crucial step in the ML pipeline and knowing how to handle these issues is essential for building robust models.

### Why Duplication and Outlier Handling Matters

**Duplicate Data Issues:**

- **Inflated Performance**: Duplicates can leak from training to test sets, causing overoptimistic results
- **Biased Models**: Overrepresentation of certain patterns can skew model learning
- **Resource Waste**: Unnecessary computational overhead and storage costs
- **Statistical Violations**: Breaks independence assumptions in many algorithms

**Outlier Impact:**

- **Model Robustness**: Outliers can disproportionately influence model parameters
- **Performance Degradation**: Can lead to poor generalization on normal data
- **Algorithm Sensitivity**: Some algorithms (like linear regression) are highly sensitive to outliers
- **Business Impact**: May represent rare but important events that need special handling

### Core Principles

1. **Understand Before Acting**: Investigate why duplicates and outliers exist
2. **Context Matters**: Domain knowledge is crucial for decision-making
3. **Preserve Information**: Sometimes outliers contain valuable insights
4. **Validate Impact**: Always assess how changes affect model performance
5. **Document Decisions**: Record rationale for reproducibility and audit trails

## üîÑ Understanding Duplicates

### Types of Duplicates

**1. Exact Duplicates**

- Identical records across all columns
- Often result from data collection errors or system glitches
- Usually safe to remove

**2. Partial Duplicates**

- Same key identifiers but different values in other columns
- May represent legitimate updates or data quality issues
- Require careful investigation

**3. Semantic Duplicates**

- Same entity represented differently (e.g., "John Smith" vs "J. Smith")
- Text variations, formatting differences, or encoding issues
- Most challenging to detect and handle

### Common Causes of Duplicates

- **Data Integration**: Merging datasets from multiple sources
- **System Errors**: Database replication issues or application bugs
- **User Error**: Manual data entry mistakes or form resubmissions
- **ETL Processes**: Extraction, transformation, and loading pipeline failures
- **API Calls**: Network timeouts leading to duplicate requests

## üéØ Duplicate Detection and Handling

### Detection Strategies

```python
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

class DuplicateDetector:
    def __init__(self, df):
        self.df = df.copy()
        self.duplicate_stats = {}
    
    def detect_exact_duplicates(self):
        """Detect exact duplicates across all columns"""
        duplicates = self.df.duplicated(keep=False)
        self.duplicate_stats['exact_duplicates'] = duplicates.sum()
        return self.df[duplicates]
    
    def detect_key_duplicates(self, key_columns):
        """Detect duplicates based on key columns"""
        duplicates = self.df.duplicated(subset=key_columns, keep=False)
        self.duplicate_stats['key_duplicates'] = duplicates.sum()
        return self.df[duplicates]
    
    def detect_fuzzy_duplicates(self, text_column, threshold=0.8):
        """Detect semantic duplicates using text similarity"""
        # Clean and vectorize text
        text_data = self.df[text_column].fillna('').astype(str)
        vectorizer = TfidfVectorizer(lowercase=True, ngram_range=(1, 2))
        tfidf_matrix = vectorizer.fit_transform(text_data)
        
        # Calculate similarity matrix
        similarity_matrix = cosine_similarity(tfidf_matrix)
        
        # Find potential duplicates
        potential_duplicates = []
        for i in range(len(similarity_matrix)):
            for j in range(i + 1, len(similarity_matrix)):
                if similarity_matrix[i][j] >= threshold:
                    potential_duplicates.append((i, j, similarity_matrix[i][j]))
        
        return potential_duplicates
    
    def get_duplicate_summary(self):
        """Generate comprehensive duplicate analysis"""
        summary = {
            'total_rows': len(self.df),
            'exact_duplicates': self.df.duplicated().sum(),
            'unique_rows': len(self.df.drop_duplicates()),
            'duplicate_percentage': (self.df.duplicated().sum() / len(self.df)) * 100
        }
        
        # Column-wise duplicate analysis
        column_duplicates = {}
        for col in self.df.columns:
            unique_count = self.df[col].nunique()
            total_count = self.df[col].count()  # Excludes NaN
            column_duplicates[col] = {
                'unique_values': unique_count,
                'total_non_null': total_count,
                'duplicate_rate': ((total_count - unique_count) / total_count) * 100 if total_count > 0 else 0
            }
        
        summary['column_analysis'] = column_duplicates
        return summary
```

### Handling Strategies

```python
class DuplicateHandler:
    def __init__(self, df):
        self.df = df.copy()
        self.original_shape = df.shape
    
    def remove_exact_duplicates(self, keep='first'):
        """Remove exact duplicates"""
        self.df = self.df.drop_duplicates(keep=keep)
        print(f"Removed {self.original_shape[0] - len(self.df)} exact duplicates")
        return self.df
    
    def remove_key_duplicates(self, key_columns, keep='first'):
        """Remove duplicates based on key columns"""
        before_count = len(self.df)
        self.df = self.df.drop_duplicates(subset=key_columns, keep=keep)
        removed_count = before_count - len(self.df)
        print(f"Removed {removed_count} duplicates based on key columns: {key_columns}")
        return self.df
    
    def handle_partial_duplicates(self, key_columns, strategy='latest'):
        """Handle partial duplicates with various strategies"""
        if strategy == 'latest':
            # Keep the latest record (assuming there's a timestamp column)
            if 'timestamp' in self.df.columns or 'date' in self.df.columns:
                timestamp_col = 'timestamp' if 'timestamp' in self.df.columns else 'date'
                self.df = self.df.sort_values(timestamp_col).drop_duplicates(
                    subset=key_columns, keep='last'
                )
        
        elif strategy == 'aggregate':
            # Aggregate numeric columns and keep first non-null for others
            numeric_cols = self.df.select_dtypes(include=[np.number]).columns
            non_numeric_cols = [col for col in self.df.columns if col not in numeric_cols and col not in key_columns]
            
            agg_dict = {}
            for col in numeric_cols:
                agg_dict[col] = 'mean'
            for col in non_numeric_cols:
                agg_dict[col] = 'first'
            
            self.df = self.df.groupby(key_columns).agg(agg_dict).reset_index()
        
        elif strategy == 'manual_review':
            # Flag for manual review
            duplicates = self.df.duplicated(subset=key_columns, keep=False)
            return self.df[duplicates].sort_values(key_columns)
        
        return self.df
```

## üìä Understanding Outliers

### Types of Outliers

**1. Point Outliers (Global)**
- Individual data points that deviate significantly from the overall distribution
- Most common type of outlier

**2. Contextual Outliers (Conditional)**
- Normal in general but anomalous in specific context
- Example: High temperature in summer vs. winter

**3. Collective Outliers**
- Individual points may be normal, but the collection is anomalous
- Example: Unusual pattern in time series data

### Business Context Considerations

- **Domain Knowledge**: What constitutes normal vs. abnormal in your field?
- **Data Collection**: Are outliers due to measurement errors or genuine phenomena?
- **Business Value**: Do outliers represent important edge cases or rare events?
- **Model Purpose**: Will the model encounter similar outliers in production?

## üîç Outlier Detection Methods

### Statistical Methods

```python
import scipy.stats as stats
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns

class OutlierDetector:
    def __init__(self, df):
        self.df = df.copy()
        self.outlier_indices = {}
    
    def detect_zscore_outliers(self, columns, threshold=3):
        """Detect outliers using Z-score method"""
        outlier_indices = set()
        
        for column in columns:
            if self.df[column].dtype in ['int64', 'float64']:
                z_scores = np.abs(stats.zscore(self.df[column].dropna()))
                column_outliers = self.df[z_scores > threshold].index
                outlier_indices.update(column_outliers)
                
                self.outlier_indices[f'{column}_zscore'] = list(column_outliers)
        
        return list(outlier_indices)
    
    def detect_iqr_outliers(self, columns, factor=1.5):
        """Detect outliers using Interquartile Range (IQR) method"""
        outlier_indices = set()
        
        for column in columns:
            if self.df[column].dtype in ['int64', 'float64']:
                Q1 = self.df[column].quantile(0.25)
                Q3 = self.df[column].quantile(0.75)
                IQR = Q3 - Q1
                
                lower_bound = Q1 - factor * IQR
                upper_bound = Q3 + factor * IQR
                
                column_outliers = self.df[
                    (self.df[column] < lower_bound) | 
                    (self.df[column] > upper_bound)
                ].index
                
                outlier_indices.update(column_outliers)
                self.outlier_indices[f'{column}_iqr'] = list(column_outliers)
        
        return list(outlier_indices)
    
    def detect_modified_zscore_outliers(self, columns, threshold=3.5):
        """Detect outliers using Modified Z-score (more robust)"""
        outlier_indices = set()
        
        for column in columns:
            if self.df[column].dtype in ['int64', 'float64']:
                median = np.median(self.df[column].dropna())
                mad = np.median(np.abs(self.df[column].dropna() - median))
                
                # Calculate modified Z-score
                modified_z_scores = 0.6745 * (self.df[column] - median) / mad
                column_outliers = self.df[np.abs(modified_z_scores) > threshold].index
                
                outlier_indices.update(column_outliers)
                self.outlier_indices[f'{column}_modified_zscore'] = list(column_outliers)
        
        return list(outlier_indices)
```

### Machine Learning Methods

```python
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from sklearn.covariance import EllipticEnvelope

class MLOutlierDetector:
    def __init__(self, df):
        self.df = df.copy()
        self.models = {}
        self.outlier_scores = {}
    
    def detect_isolation_forest(self, features, contamination=0.1):
        """Detect outliers using Isolation Forest"""
        model = IsolationForest(contamination=contamination, random_state=42)
        outlier_predictions = model.fit_predict(self.df[features])
        
        # -1 indicates outlier, 1 indicates normal
        outlier_indices = self.df[outlier_predictions == -1].index
        
        self.models['isolation_forest'] = model
        self.outlier_scores['isolation_forest'] = model.decision_function(self.df[features])
        
        return list(outlier_indices)
    
    def detect_local_outlier_factor(self, features, n_neighbors=20):
        """Detect outliers using Local Outlier Factor"""
        model = LocalOutlierFactor(n_neighbors=n_neighbors)
        outlier_predictions = model.fit_predict(self.df[features])
        
        outlier_indices = self.df[outlier_predictions == -1].index
        
        self.models['lof'] = model
        self.outlier_scores['lof'] = model.negative_outlier_factor_
        
        return list(outlier_indices)
    
    def detect_one_class_svm(self, features, nu=0.1):
        """Detect outliers using One-Class SVM"""
        model = OneClassSVM(nu=nu)
        outlier_predictions = model.fit_predict(self.df[features])
        
        outlier_indices = self.df[outlier_predictions == -1].index
        
        self.models['one_class_svm'] = model
        self.outlier_scores['one_class_svm'] = model.decision_function(self.df[features])
        
        return list(outlier_indices)
    
    def detect_elliptic_envelope(self, features, contamination=0.1):
        """Detect outliers using Elliptic Envelope (assumes Gaussian distribution)"""
        model = EllipticEnvelope(contamination=contamination, random_state=42)
        outlier_predictions = model.fit_predict(self.df[features])
        
        outlier_indices = self.df[outlier_predictions == -1].index
        
        self.models['elliptic_envelope'] = model
        self.outlier_scores['elliptic_envelope'] = model.decision_function(self.df[features])
        
        return list(outlier_indices)
    
    def ensemble_detection(self, features, methods=['isolation_forest', 'lof'], 
                          threshold=0.5):
        """Combine multiple methods for robust outlier detection"""
        all_outliers = []
        
        if 'isolation_forest' in methods:
            all_outliers.extend(self.detect_isolation_forest(features))
        if 'lof' in methods:
            all_outliers.extend(self.detect_local_outlier_factor(features))
        if 'one_class_svm' in methods:
            all_outliers.extend(self.detect_one_class_svm(features))
        if 'elliptic_envelope' in methods:
            all_outliers.extend(self.detect_elliptic_envelope(features))
        
        # Count votes for each index
        outlier_counts = pd.Series(all_outliers).value_counts()
        
        # Keep outliers that were detected by at least threshold proportion of methods
        min_votes = int(len(methods) * threshold)
        consensus_outliers = outlier_counts[outlier_counts >= min_votes].index
        
        return list(consensus_outliers)
```

## üõ†Ô∏è Outlier Handling Strategies

```python
class OutlierHandler:
    def __init__(self, df):
        self.df = df.copy()
        self.original_shape = df.shape
        self.handling_log = []
    
    def remove_outliers(self, outlier_indices):
        """Remove outliers from dataset"""
        self.df = self.df.drop(outlier_indices)
        removed_count = len(outlier_indices)
        
        self.handling_log.append({
            'method': 'removal',
            'count': removed_count,
            'indices': outlier_indices
        })
        
        print(f"Removed {removed_count} outliers")
        return self.df
    
    def cap_outliers(self, column, method='iqr', factor=1.5):
        """Cap outliers to reasonable bounds"""
        original_outliers = self.df[column].copy()
        
        if method == 'iqr':
            Q1 = self.df[column].quantile(0.25)
            Q3 = self.df[column].quantile(0.75)
            IQR = Q3 - Q1
            
            lower_bound = Q1 - factor * IQR
            upper_bound = Q3 + factor * IQR
            
        elif method == 'percentile':
            lower_bound = self.df[column].quantile(0.01)
            upper_bound = self.df[column].quantile(0.99)
        
        # Cap the values
        self.df[column] = self.df[column].clip(lower=lower_bound, upper=upper_bound)
        
        # Log changes
        changed_count = (original_outliers != self.df[column]).sum()
        self.handling_log.append({
            'method': f'capping_{method}',
            'column': column,
            'count': changed_count,
            'bounds': (lower_bound, upper_bound)
        })
        
        print(f"Capped {changed_count} outliers in column '{column}'")
        return self.df
    
    def transform_outliers(self, columns, method='log'):
        """Transform data to reduce outlier impact"""
        for column in columns:
            if self.df[column].dtype in ['int64', 'float64']:
                original_data = self.df[column].copy()
                
                if method == 'log':
                    # Add 1 to handle zero values
                    self.df[column] = np.log1p(self.df[column])
                
                elif method == 'sqrt':
                    # Handle negative values
                    self.df[column] = np.sign(self.df[column]) * np.sqrt(np.abs(self.df[column]))
                
                elif method == 'box_cox':
                    from scipy.stats import boxcox
                    # Box-Cox requires positive values
                    if self.df[column].min() > 0:
                        self.df[column], _ = boxcox(self.df[column])
                    else:
                        print(f"Skipping Box-Cox for {column}: contains non-positive values")
                        continue
                
                elif method == 'yeo_johnson':
                    from scipy.stats import yeojohnson
                    self.df[column], _ = yeojohnson(self.df[column])
                
                self.handling_log.append({
                    'method': f'transform_{method}',
                    'column': column,
                    'original_skew': stats.skew(original_data.dropna()),
                    'new_skew': stats.skew(self.df[column].dropna())
                })
        
        return self.df
    
    def impute_outliers(self, outlier_indices, columns, method='median'):
        """Replace outliers with imputed values"""
        for column in columns:
            if column in self.df.columns:
                original_values = self.df.loc[outlier_indices, column].copy()
                
                if method == 'median':
                    fill_value = self.df[column].median()
                elif method == 'mean':
                    fill_value = self.df[column].mean()
                elif method == 'mode':
                    fill_value = self.df[column].mode().iloc[0]
                
                self.df.loc[outlier_indices, column] = fill_value
                
                changed_count = len(outlier_indices)
                self.handling_log.append({
                    'method': f'impute_{method}',
                    'column': column,
                    'count': changed_count,
                    'fill_value': fill_value
                })
        
        return self.df

## üíª Code Examples and Implementation

### Complete Pipeline Example

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

class DataQualityPipeline:
    def __init__(self, df):
        self.df = df.copy()
        self.original_shape = df.shape
        self.processing_log = []
    
    def comprehensive_duplicate_analysis(self):
        """Perform comprehensive duplicate analysis"""
        detector = DuplicateDetector(self.df)
        
        print("=== DUPLICATE ANALYSIS ===")
        summary = detector.get_duplicate_summary()
        
        print(f"Total rows: {summary['total_rows']}")
        print(f"Exact duplicates: {summary['exact_duplicates']}")
        print(f"Unique rows: {summary['unique_rows']}")
        print(f"Duplicate percentage: {summary['duplicate_percentage']:.2f}%")
        
        # Column-wise analysis
        print("\n--- Column-wise Duplicate Analysis ---")
        for col, stats in summary['column_analysis'].items():
            print(f"{col}: {stats['unique_values']} unique / {stats['total_non_null']} total "
                  f"(Duplicate rate: {stats['duplicate_rate']:.1f}%)")
        
        return summary
    
    def comprehensive_outlier_analysis(self, numerical_columns):
        """Perform comprehensive outlier analysis"""
        print("\n=== OUTLIER ANALYSIS ===")
        
        # Statistical methods
        stat_detector = OutlierDetector(self.df)
        zscore_outliers = stat_detector.detect_zscore_outliers(numerical_columns)
        iqr_outliers = stat_detector.detect_iqr_outliers(numerical_columns)
        
        # ML methods
        ml_detector = MLOutlierDetector(self.df)
        iso_outliers = ml_detector.detect_isolation_forest(numerical_columns)
        ensemble_outliers = ml_detector.ensemble_detection(
            numerical_columns, 
            methods=['isolation_forest', 'lof'], 
            threshold=0.5
        )
        
        print(f"Z-score outliers: {len(zscore_outliers)}")
        print(f"IQR outliers: {len(iqr_outliers)}")
        print(f"Isolation Forest outliers: {len(iso_outliers)}")
        print(f"Ensemble outliers: {len(ensemble_outliers)}")
        
        return {
            'zscore': zscore_outliers,
            'iqr': iqr_outliers,
            'isolation_forest': iso_outliers,
            'ensemble': ensemble_outliers
        }
    
    def visualize_outliers(self, numerical_columns):
        """Create comprehensive outlier visualizations"""
        n_cols = len(numerical_columns)
        fig, axes = plt.subplots(2, n_cols, figsize=(5*n_cols, 10))
        
        if n_cols == 1:
            axes = axes.reshape(-1, 1)
        
        for i, col in enumerate(numerical_columns):
            # Box plot
            axes[0, i].boxplot(self.df[col].dropna())
            axes[0, i].set_title(f'Box Plot: {col}')
            axes[0, i].grid(True, alpha=0.3)
            
            # Histogram with normal distribution overlay
            axes[1, i].hist(self.df[col].dropna(), bins=30, alpha=0.7, density=True)
            
            # Add normal distribution overlay
            mu, sigma = self.df[col].mean(), self.df[col].std()
            x = np.linspace(self.df[col].min(), self.df[col].max(), 100)
            axes[1, i].plot(x, stats.norm.pdf(x, mu, sigma), 'r-', linewidth=2, 
                           label='Normal Distribution')
            axes[1, i].set_title(f'Distribution: {col}')
            axes[1, i].legend()
            axes[1, i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def apply_data_quality_pipeline(self, numerical_columns, 
                                   duplicate_strategy='remove_exact',
                                   outlier_strategy='cap_iqr'):
        """Apply complete data quality pipeline"""
        print("=== APPLYING DATA QUALITY PIPELINE ===")
        
        # Handle duplicates
        handler = DuplicateHandler(self.df)
        
        if duplicate_strategy == 'remove_exact':
            self.df = handler.remove_exact_duplicates()
        elif duplicate_strategy == 'remove_key':
            # This would need key columns specified
            pass
        
        # Handle outliers
        outlier_handler = OutlierHandler(self.df)
        
        if outlier_strategy == 'remove':
            # Detect outliers first
            detector = OutlierDetector(self.df)
            outliers = detector.detect_iqr_outliers(numerical_columns)
            self.df = outlier_handler.remove_outliers(outliers)
        
        elif outlier_strategy == 'cap_iqr':
            for col in numerical_columns:
                self.df = outlier_handler.cap_outliers(col, method='iqr')
        
        elif outlier_strategy == 'transform_log':
            self.df = outlier_handler.transform_outliers(numerical_columns, method='log')
        
        print(f"Final shape: {self.df.shape} (Original: {self.original_shape})")
        
        return self.df

# Example usage
def run_complete_example():
    # Create sample data with duplicates and outliers
    np.random.seed(42)
    n_samples = 1000
    
    data = {
        'id': range(n_samples),
        'feature1': np.random.normal(50, 15, n_samples),
        'feature2': np.random.exponential(2, n_samples),
        'target': np.random.normal(100, 25, n_samples)
    }
    
    df = pd.DataFrame(data)
    
    # Add some duplicates
    duplicates = df.sample(50).copy()
    df = pd.concat([df, duplicates], ignore_index=True)
    
    # Add some outliers
    outlier_indices = np.random.choice(df.index, 20, replace=False)
    df.loc[outlier_indices, 'feature1'] = df['feature1'].mean() + 5 * df['feature1'].std()
    df.loc[outlier_indices, 'target'] = df['target'].mean() + 4 * df['target'].std()
    
    # Run pipeline
    pipeline = DataQualityPipeline(df)
    
    # Analysis
    duplicate_summary = pipeline.comprehensive_duplicate_analysis()
    outlier_summary = pipeline.comprehensive_outlier_analysis(['feature1', 'feature2', 'target'])
    
    # Visualization
    pipeline.visualize_outliers(['feature1', 'feature2', 'target'])
    
    # Apply cleaning
    cleaned_df = pipeline.apply_data_quality_pipeline(
        numerical_columns=['feature1', 'feature2', 'target'],
        duplicate_strategy='remove_exact',
        outlier_strategy='cap_iqr'
    )
    
    return cleaned_df

# Run the example
# cleaned_data = run_complete_example()
```

### Model Performance Comparison

```python
def compare_model_performance(original_df, cleaned_df, target_column, feature_columns):
    """Compare model performance before and after data quality improvements"""
    
    results = {}
    
    for name, df in [('Original', original_df), ('Cleaned', cleaned_df)]:
        # Prepare data
        X = df[feature_columns]
        y = df[target_column]
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # Train model
        model = RandomForestRegressor(n_estimators=100, random_state=42)
        model.fit(X_train, y_train)
        
        # Evaluate
        y_pred = model.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        
        results[name] = {
            'MSE': mse,
            'R2': r2,
            'Data_Shape': df.shape
        }
    
    # Display results
    print("=== MODEL PERFORMANCE COMPARISON ===")
    for name, metrics in results.items():
        print(f"\n{name} Dataset:")
        print(f"  Shape: {metrics['Data_Shape']}")
        print(f"  MSE: {metrics['MSE']:.4f}")
        print(f"  R¬≤: {metrics['R2']:.4f}")
    
    # Calculate improvement
    mse_improvement = ((results['Original']['MSE'] - results['Cleaned']['MSE']) / 
                      results['Original']['MSE']) * 100
    r2_improvement = ((results['Cleaned']['R2'] - results['Original']['R2']) / 
                     results['Original']['R2']) * 100
    
    print(f"\nImprovement Summary:")
    print(f"  MSE improvement: {mse_improvement:.2f}%")
    print(f"  R¬≤ improvement: {r2_improvement:.2f}%")
    
    return results
```

## ‚úÖ Validation and Quality Checks

### Data Quality Metrics

```python
class DataQualityValidator:
    def __init__(self, original_df, processed_df):
        self.original_df = original_df
        self.processed_df = processed_df
    
    def validate_data_integrity(self):
        """Validate that data processing maintained integrity"""
        checks = {}
        
        # Shape comparison
        checks['shape_change'] = {
            'original': self.original_df.shape,
            'processed': self.processed_df.shape,
            'rows_removed': self.original_df.shape[0] - self.processed_df.shape[0],
            'columns_changed': self.original_df.shape[1] != self.processed_df.shape[1]
        }
        
        # Missing data comparison
        checks['missing_data'] = {
            'original_missing': self.original_df.isnull().sum().sum(),
            'processed_missing': self.processed_df.isnull().sum().sum()
        }
        
        # Data type consistency
        original_dtypes = set(self.original_df.dtypes.astype(str))
        processed_dtypes = set(self.processed_df.dtypes.astype(str))
        checks['dtype_consistency'] = original_dtypes == processed_dtypes
        
        # Value range changes
        numerical_cols = self.original_df.select_dtypes(include=[np.number]).columns
        checks['value_ranges'] = {}
        
        for col in numerical_cols:
            if col in self.processed_df.columns:
                orig_range = (self.original_df[col].min(), self.original_df[col].max())
                proc_range = (self.processed_df[col].min(), self.processed_df[col].max())
                checks['value_ranges'][col] = {
                    'original': orig_range,
                    'processed': proc_range,
                    'range_preserved': orig_range == proc_range
                }
        
        return checks
    
    def generate_quality_report(self):
        """Generate comprehensive data quality report"""
        report = {
            'processing_summary': {
                'original_rows': len(self.original_df),
                'processed_rows': len(self.processed_df),
                'rows_removed': len(self.original_df) - len(self.processed_df),
                'removal_percentage': ((len(self.original_df) - len(self.processed_df)) / 
                                     len(self.original_df)) * 100
            }
        }
        
        # Duplicate analysis
        report['duplicates'] = {
            'original_duplicates': self.original_df.duplicated().sum(),
            'processed_duplicates': self.processed_df.duplicated().sum()
        }
        
        # Statistical summary comparison
        numerical_cols = self.original_df.select_dtypes(include=[np.number]).columns
        report['statistical_changes'] = {}
        
        for col in numerical_cols:
            if col in self.processed_df.columns:
                orig_stats = self.original_df[col].describe()
                proc_stats = self.processed_df[col].describe()
                
                report['statistical_changes'][col] = {
                    'mean_change': proc_stats['mean'] - orig_stats['mean'],
                    'std_change': proc_stats['std'] - orig_stats['std'],
                    'median_change': proc_stats['50%'] - orig_stats['50%']
                }
        
        return report
    
    def create_before_after_visualization(self, columns):
        """Create before/after visualizations"""
        n_cols = len(columns)
        fig, axes = plt.subplots(2, n_cols, figsize=(5*n_cols, 10))
        
        if n_cols == 1:
            axes = axes.reshape(-1, 1)
        
        for i, col in enumerate(columns):
            # Original data
            axes[0, i].hist(self.original_df[col].dropna(), bins=30, alpha=0.7, 
                           color='red', label='Original')
            axes[0, i].set_title(f'Original: {col}')
            axes[0, i].legend()
            axes[0, i].grid(True, alpha=0.3)
            
            # Processed data
            axes[1, i].hist(self.processed_df[col].dropna(), bins=30, alpha=0.7, 
                           color='blue', label='Processed')
            axes[1, i].set_title(f'Processed: {col}')
            axes[1, i].legend()
            axes[1, i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
```

## ‚≠ê Best Practices

### 1. **Systematic Approach**

```python
# Always follow a structured approach
def data_quality_checklist():
    """Systematic data quality assessment checklist"""
    checklist = [
        "‚úì Understand data source and collection process",
        "‚úì Perform initial data profiling and statistics",
        "‚úì Identify and quantify duplicate patterns",
        "‚úì Analyze outlier distributions and causes",
        "‚úì Consider business context and domain knowledge",
        "‚úì Document all assumptions and decisions",
        "‚úì Validate impact on model performance",
        "‚úì Create reproducible processing pipeline",
        "‚úì Monitor data quality over time"
    ]
    
    for item in checklist:
        print(item)
```

### 2. **Documentation and Reproducibility**

```python
class DataProcessingLogger:
    def __init__(self):
        self.log = []
        self.timestamp = pd.Timestamp.now()
    
    def log_operation(self, operation, details, impact):
        """Log data processing operations"""
        entry = {
            'timestamp': pd.Timestamp.now(),
            'operation': operation,
            'details': details,
            'impact': impact
        }
        self.log.append(entry)
    
    def export_log(self, filename):
        """Export processing log for reproducibility"""
        log_df = pd.DataFrame(self.log)
        log_df.to_csv(filename, index=False)
        
    def generate_summary_report(self):
        """Generate human-readable summary"""
        report = f"""
        Data Processing Summary
        =====================
        Processing Date: {self.timestamp}
        Total Operations: {len(self.log)}
        
        Operations Performed:
        """
        
        for i, entry in enumerate(self.log, 1):
            report += f"\n{i}. {entry['operation']}: {entry['details']}"
            report += f"\n   Impact: {entry['impact']}\n"
        
        return report
```

### 3. **Domain-Specific Considerations**

- **Financial Data**: Regulatory compliance, audit trails, conservative outlier handling
- **Healthcare Data**: Patient privacy, clinical significance of outliers
- **IoT/Sensor Data**: Temporal context, equipment failure patterns
- **E-commerce**: Seasonal patterns, promotional impacts
- **Social Media**: Bot detection, viral content patterns

### 4. **Performance Monitoring**

```python
def monitor_data_quality_trends(data_sources, time_column):
    """Monitor data quality trends over time"""
    quality_metrics = []
    
    for source_name, df in data_sources.items():
        # Group by time periods
        df['period'] = pd.to_datetime(df[time_column]).dt.to_period('M')
        
        for period, group in df.groupby('period'):
            metrics = {
                'source': source_name,
                'period': str(period),
                'total_records': len(group),
                'duplicate_rate': group.duplicated().mean() * 100,
                'missing_rate': group.isnull().mean().mean() * 100
            }
            
            # Outlier rates for numerical columns
            numerical_cols = group.select_dtypes(include=[np.number]).columns
            outlier_rates = []
            
            for col in numerical_cols:
                Q1 = group[col].quantile(0.25)
                Q3 = group[col].quantile(0.75)
                IQR = Q3 - Q1
                outliers = ((group[col] < Q1 - 1.5*IQR) | 
                           (group[col] > Q3 + 1.5*IQR)).mean()
                outlier_rates.append(outliers * 100)
            
            metrics['avg_outlier_rate'] = np.mean(outlier_rates) if outlier_rates else 0
            quality_metrics.append(metrics)
    
    return pd.DataFrame(quality_metrics)
```

## ‚ö†Ô∏è Common Pitfalls

### 1. **Over-aggressive Outlier Removal**

```python
# ‚ùå Wrong: Removing all outliers without consideration
def bad_outlier_handling(df, columns):
    for col in columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        df = df[(df[col] >= Q1 - 1.5*IQR) & (df[col] <= Q3 + 1.5*IQR)]
    return df

# ‚úÖ Better: Consider business context and model impact
def thoughtful_outlier_handling(df, columns, target_column=None):
    outlier_analysis = {}
    
    for col in columns:
        # Detect outliers
        detector = OutlierDetector(df)
        outliers = detector.detect_iqr_outliers([col])
        
        # Analyze outlier characteristics
        outlier_data = df.loc[outliers]
        
        analysis = {
            'count': len(outliers),
            'percentage': len(outliers) / len(df) * 100,
            'value_range': (outlier_data[col].min(), outlier_data[col].max())
        }
        
        # If target column exists, check outlier impact on target
        if target_column and target_column in df.columns:
            normal_target_mean = df.drop(outliers)[target_column].mean()
            outlier_target_mean = outlier_data[target_column].mean()
            analysis['target_difference'] = abs(outlier_target_mean - normal_target_mean)
        
        outlier_analysis[col] = analysis
        
        # Make informed decision
        if analysis['percentage'] > 5:  # More than 5% are outliers
            print(f"Warning: {col} has {analysis['percentage']:.1f}% outliers")
            print("Consider domain expertise before removal")
    
    return outlier_analysis
```

### 2. **Ignoring Data Leakage in Train/Test Splits**

```python
# ‚ùå Wrong: Removing duplicates after train/test split
def bad_duplicate_handling(df, target_col):
    X = df.drop(target_col, axis=1)
    y = df[target_col]
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    
    # This creates data leakage!
    train_df = pd.concat([X_train, y_train], axis=1)
    test_df = pd.concat([X_test, y_test], axis=1)
    
    train_df = train_df.drop_duplicates()
    test_df = test_df.drop_duplicates()
    
    return train_df, test_df

# ‚úÖ Correct: Remove duplicates before splitting
def proper_duplicate_handling(df, target_col):
    # Remove duplicates first
    df_clean = df.drop_duplicates()
    
    # Then split
    X = df_clean.drop(target_col, axis=1)
    y = df_clean[target_col]
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    
    return X_train, X_test, y_train, y_test
```

### 3. **Not Validating Assumptions**

```python
# Always validate your assumptions about data quality
def validate_outlier_assumptions(df, column):
    """Validate assumptions about outlier detection methods"""
    
    # Check distribution normality (important for Z-score)
    normality_stat, normality_p = stats.normaltest(df[column].dropna())
    is_normal = normality_p > 0.05
    
    # Check for skewness
    skewness = stats.skew(df[column].dropna())
    
    # Recommendations
    recommendations = []
    
    if not is_normal:
        recommendations.append("Data is not normally distributed - consider Modified Z-score or IQR")
    
    if abs(skewness) > 1:
        recommendations.append("Data is highly skewed - consider transformation before outlier detection")
    
    if df[column].min() <= 0 and "log" in str(df[column].dtype):
        recommendations.append("Data contains non-positive values - log transformation not suitable")
    
    return {
        'is_normal': is_normal,
        'skewness': skewness,
        'recommendations': recommendations
    }
```

### 4. **Inadequate Documentation**

```python
# ‚úÖ Always document your decisions
def document_data_quality_decisions():
    """Example of proper documentation structure"""
    
    documentation = {
        'duplicate_handling': {
            'method': 'exact_removal',
            'rationale': 'Duplicates were system errors from data pipeline',
            'count_removed': 150,
            'verification': 'Confirmed with data engineering team'
        },
        
        'outlier_handling': {
            'detection_method': 'IQR with factor=1.5',
            'handling_strategy': 'capping',
            'rationale': 'Outliers represent valid but extreme values',
            'business_impact': 'Preserves rare but important customer segments',
            'validation': 'Model performance improved by 5% on test set'
        },
        
        'assumptions': [
            'Data collection process is stable',
            'Outliers are measurement errors, not genuine extreme values',
            'Missing data is random (MCAR)'
        ],
        
        'risks': [
            'May lose information about rare events',
            'Assumptions about data distribution may not hold',
            'Processing may not generalize to future data'
        ]
    }
    
    return documentation
```

## üìù Summary

Effective duplication and outlier handling is crucial for building robust machine learning models. This guide provided a comprehensive framework covering:

### Key Takeaways

1. **Understanding is Critical**: Always investigate the root causes of duplicates and outliers before taking action

2. **Multiple Methods**: Use various detection techniques and compare results for robust identification

3. **Context Matters**: Business domain knowledge should guide handling decisions more than statistical rules

4. **Validation is Essential**: Always assess the impact of data quality improvements on model performance

5. **Documentation**: Maintain detailed records of all decisions and rationale for reproducibility

### Decision Framework

When encountering duplicates or outliers, ask:

- **Why do they exist?** (Data quality issue vs. genuine phenomenon)
- **What is the business impact?** (Cost of false positives vs. false negatives)
- **How will this affect my model?** (Robustness vs. performance trade-offs)
- **Is this generalizable?** (Will similar patterns appear in production?)

### Next Steps

With clean, high-quality data, you're ready to move to the next phase of your ML pipeline:

- **Feature Engineering**: Creating new features from cleaned data
- **Feature Selection**: Identifying the most relevant variables
- **Model Selection**: Choosing appropriate algorithms for your clean dataset
- **Performance Monitoring**: Tracking data quality in production

Remember: Data quality is not a one-time task but an ongoing process that requires continuous monitoring and adjustment as your data and business requirements evolve.

---

**Related Guides in This Series:**
- [Exploratory Data Analysis](./exploratory_data_analysis.md)
- [Missing Data Imputation](./missing_data_imputation.md)
- Feature Engineering and Selection (coming soon)
- Model Selection and Validation (coming soon)
```
