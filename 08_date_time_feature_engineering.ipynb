{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "652f9b95",
   "metadata": {},
   "source": [
    "# ⏰ **Phase 8: Date & Time Feature Engineering**\n",
    "\n",
    "This notebook extracts temporal features from date/time columns to enhance machine learning model performance.\n",
    "\n",
    "## 🎯 **Temporal Feature Engineering Objectives**\n",
    "\n",
    "Transform date and time data into meaningful features for machine learning:\n",
    "\n",
    "1. **📅 Basic Temporal Components** - Extract year, month, day, weekday\n",
    "2. **🗓️ Cyclical Features** - Create sine/cosine transformations for periodic patterns\n",
    "3. **📊 Business Patterns** - Identify seasonal trends and business cycles\n",
    "4. **⏱️ Time-based Aggregations** - Calculate rolling windows and lag features\n",
    "5. **🎪 Holiday & Event Features** - Flag special dates and periods\n",
    "6. **📈 Trend Analysis** - Extract time-based trends and patterns\n",
    "\n",
    "### 🔄 **Input/Output**\n",
    "- **Input**: Cleaned datasets from Phase 7 (`data_intermediate/07_outlier_handled_dataframes.pkl`)\n",
    "- **Output**: Datasets with temporal features (`data_intermediate/08_temporal_features_data.pkl`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c11f043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏰ DATE & TIME FEATURE ENGINEERING - PHASE 8\n",
      "============================================================\n",
      "📚 Libraries loaded successfully\n",
      "🎯 Ready to engineer temporal features\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"⏰ DATE & TIME FEATURE ENGINEERING - PHASE 8\")\n",
    "print(\"=\" * 60)\n",
    "print(\"📚 Libraries loaded successfully\")\n",
    "print(\"🎯 Ready to engineer temporal features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f4f8218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 LOADING DATA FROM PREVIOUS PHASE\n",
      "==================================================\n",
      "✅ Successfully loaded 9 datasets\n",
      "   📊 details_adventure_gear: 106 rows, 24 columns\n",
      "   📊 details_magic_items: 199 rows, 22 columns\n",
      "   📊 details_weapons: 37 rows, 27 columns\n",
      "   📊 details_armor: 13 rows, 44 columns\n",
      "   📊 details_potions: 22 rows, 17 columns\n",
      "   📊 details_poisons: 16 rows, 20 columns\n",
      "   📊 all_products: 393 rows, 18 columns\n",
      "   📊 customers: 1,423 rows, 17 columns\n",
      "   📊 sales: 54,126 rows, 24 columns\n",
      "\n",
      "📈 Total records: 56,335\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned data from previous phase\n",
    "print(\"📂 LOADING DATA FROM PREVIOUS PHASE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    with open('data_intermediate/07_outlier_handled_dataframes.pkl', 'rb') as f:\n",
    "        dataframes = pickle.load(f)\n",
    "    \n",
    "    print(f\"✅ Successfully loaded {len(dataframes)} datasets\")\n",
    "    \n",
    "    # Display dataset info\n",
    "    total_rows = 0\n",
    "    for name, df in dataframes.items():\n",
    "        total_rows += len(df)\n",
    "        print(f\"   📊 {name}: {len(df):,} rows, {len(df.columns)} columns\")\n",
    "    \n",
    "    print(f\"\\n📈 Total records: {total_rows:,}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Previous phase data not found. Please run Phase 7 first.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6120aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 IDENTIFYING DATE/TIME COLUMNS\n",
      "==================================================\n",
      "\n",
      "📋 Analyzing: details_adventure_gear\n",
      "------------------------------\n",
      "   ❌ No obvious date/time columns found\n",
      "\n",
      "📋 Analyzing: details_magic_items\n",
      "------------------------------\n",
      "   ❌ No obvious date/time columns found\n",
      "\n",
      "📋 Analyzing: details_weapons\n",
      "------------------------------\n",
      "   ❌ No obvious date/time columns found\n",
      "\n",
      "📋 Analyzing: details_armor\n",
      "------------------------------\n",
      "   ❌ No obvious date/time columns found\n",
      "\n",
      "📋 Analyzing: details_potions\n",
      "------------------------------\n",
      "   ❌ No obvious date/time columns found\n",
      "\n",
      "📋 Analyzing: details_poisons\n",
      "------------------------------\n",
      "   ❌ No obvious date/time columns found\n",
      "\n",
      "📋 Analyzing: all_products\n",
      "------------------------------\n",
      "   ❌ No obvious date/time columns found\n",
      "\n",
      "📋 Analyzing: customers\n",
      "------------------------------\n",
      "   ❌ No obvious date/time columns found\n",
      "\n",
      "📋 Analyzing: sales\n",
      "------------------------------\n",
      "   📅 Found potential date column: date\n",
      "      Data type: object\n",
      "      Non-null values: 54,126\n",
      "      Sample values: ['other', 'other', 'other']\n",
      "   📅 Found potential date column: date_freq\n",
      "      Data type: UInt16\n",
      "      Non-null values: 54,126\n",
      "      Sample values: [54126, 54126, 54126]\n",
      "   📅 Found potential date column: date_label\n",
      "      Data type: float32\n",
      "      Non-null values: 54,126\n",
      "      Sample values: [0.0, 0.0, 0.0]\n",
      "\n",
      "📊 Summary: Found 3 potential date/time columns\n",
      "   • sales.date\n",
      "   • sales.date_freq\n",
      "   • sales.date_label\n"
     ]
    }
   ],
   "source": [
    "# Identify date/time columns across all datasets\n",
    "print(\"🔍 IDENTIFYING DATE/TIME COLUMNS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "datetime_columns = {}\n",
    "potential_date_cols = []\n",
    "\n",
    "for table_name, df in dataframes.items():\n",
    "    print(f\"\\n📋 Analyzing: {table_name}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Look for datetime columns by name patterns\n",
    "    date_candidates = [col for col in df.columns if any(word in col.lower() \n",
    "                      for word in ['date', 'time', 'created', 'updated', 'timestamp'])]\n",
    "    \n",
    "    if date_candidates:\n",
    "        datetime_columns[table_name] = []\n",
    "        \n",
    "        for col in date_candidates:\n",
    "            print(f\"   📅 Found potential date column: {col}\")\n",
    "            print(f\"      Data type: {df[col].dtype}\")\n",
    "            print(f\"      Non-null values: {df[col].notna().sum():,}\")\n",
    "            \n",
    "            # Sample some values\n",
    "            sample_values = df[col].dropna().head(3).tolist()\n",
    "            print(f\"      Sample values: {sample_values}\")\n",
    "            \n",
    "            datetime_columns[table_name].append(col)\n",
    "            potential_date_cols.append(f\"{table_name}.{col}\")\n",
    "    else:\n",
    "        print(f\"   ❌ No obvious date/time columns found\")\n",
    "\n",
    "print(f\"\\n📊 Summary: Found {len(potential_date_cols)} potential date/time columns\")\n",
    "for col in potential_date_cols:\n",
    "    print(f\"   • {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88bec722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛠️ DEFINING TEMPORAL FEATURE ENGINEERING FUNCTIONS\n",
      "============================================================\n",
      "✅ Temporal feature engineering functions defined\n",
      "   📅 extract_temporal_features - Basic & cyclical features\n",
      "   📊 create_lag_features - Historical lag features\n",
      "   🔄 create_rolling_features - Rolling window statistics\n"
     ]
    }
   ],
   "source": [
    "# Define temporal feature engineering functions\n",
    "print(\"🛠️ DEFINING TEMPORAL FEATURE ENGINEERING FUNCTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def extract_temporal_features(df, date_col, prefix=''):\n",
    "    \"\"\"\n",
    "    Extract comprehensive temporal features from a datetime column\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe\n",
    "    date_col : str\n",
    "        Name of the date column\n",
    "    prefix : str\n",
    "        Prefix for new column names\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Dataframe with added temporal features\n",
    "    \"\"\"\n",
    "    df_temp = df.copy()\n",
    "    \n",
    "    # Ensure column is datetime\n",
    "    try:\n",
    "        df_temp[date_col] = pd.to_datetime(df_temp[date_col])\n",
    "    except:\n",
    "        print(f\"   ⚠️  Could not convert {date_col} to datetime\")\n",
    "        return df_temp\n",
    "    \n",
    "    prefix = f\"{prefix}_\" if prefix else \"\"\n",
    "    \n",
    "    # Basic temporal components\n",
    "    df_temp[f'{prefix}year'] = df_temp[date_col].dt.year\n",
    "    df_temp[f'{prefix}month'] = df_temp[date_col].dt.month\n",
    "    df_temp[f'{prefix}day'] = df_temp[date_col].dt.day\n",
    "    df_temp[f'{prefix}weekday'] = df_temp[date_col].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "    df_temp[f'{prefix}hour'] = df_temp[date_col].dt.hour\n",
    "    df_temp[f'{prefix}quarter'] = df_temp[date_col].dt.quarter\n",
    "    df_temp[f'{prefix}day_of_year'] = df_temp[date_col].dt.dayofyear\n",
    "    df_temp[f'{prefix}week_of_year'] = df_temp[date_col].dt.isocalendar().week\n",
    "    \n",
    "    # Cyclical features (sine/cosine transformations)\n",
    "    # Month cyclical (12 months)\n",
    "    df_temp[f'{prefix}month_sin'] = np.sin(2 * np.pi * df_temp[f'{prefix}month'] / 12)\n",
    "    df_temp[f'{prefix}month_cos'] = np.cos(2 * np.pi * df_temp[f'{prefix}month'] / 12)\n",
    "    \n",
    "    # Day of week cyclical (7 days)\n",
    "    df_temp[f'{prefix}weekday_sin'] = np.sin(2 * np.pi * df_temp[f'{prefix}weekday'] / 7)\n",
    "    df_temp[f'{prefix}weekday_cos'] = np.cos(2 * np.pi * df_temp[f'{prefix}weekday'] / 7)\n",
    "    \n",
    "    # Hour cyclical (24 hours)\n",
    "    df_temp[f'{prefix}hour_sin'] = np.sin(2 * np.pi * df_temp[f'{prefix}hour'] / 24)\n",
    "    df_temp[f'{prefix}hour_cos'] = np.cos(2 * np.pi * df_temp[f'{prefix}hour'] / 24)\n",
    "    \n",
    "    # Day of year cyclical (365 days)\n",
    "    df_temp[f'{prefix}day_of_year_sin'] = np.sin(2 * np.pi * df_temp[f'{prefix}day_of_year'] / 365)\n",
    "    df_temp[f'{prefix}day_of_year_cos'] = np.cos(2 * np.pi * df_temp[f'{prefix}day_of_year'] / 365)\n",
    "    \n",
    "    # Business time features\n",
    "    df_temp[f'{prefix}is_weekend'] = (df_temp[f'{prefix}weekday'] >= 5).astype(int)\n",
    "    df_temp[f'{prefix}is_monday'] = (df_temp[f'{prefix}weekday'] == 0).astype(int)\n",
    "    df_temp[f'{prefix}is_friday'] = (df_temp[f'{prefix}weekday'] == 4).astype(int)\n",
    "    \n",
    "    # Seasonal features\n",
    "    df_temp[f'{prefix}is_spring'] = df_temp[f'{prefix}month'].isin([3, 4, 5]).astype(int)\n",
    "    df_temp[f'{prefix}is_summer'] = df_temp[f'{prefix}month'].isin([6, 7, 8]).astype(int)\n",
    "    df_temp[f'{prefix}is_autumn'] = df_temp[f'{prefix}month'].isin([9, 10, 11]).astype(int)\n",
    "    df_temp[f'{prefix}is_winter'] = df_temp[f'{prefix}month'].isin([12, 1, 2]).astype(int)\n",
    "    \n",
    "    # Month-specific business features\n",
    "    df_temp[f'{prefix}is_january'] = (df_temp[f'{prefix}month'] == 1).astype(int)\n",
    "    df_temp[f'{prefix}is_december'] = (df_temp[f'{prefix}month'] == 12).astype(int)\n",
    "    df_temp[f'{prefix}is_quarter_end'] = df_temp[f'{prefix}month'].isin([3, 6, 9, 12]).astype(int)\n",
    "    df_temp[f'{prefix}is_quarter_start'] = df_temp[f'{prefix}month'].isin([1, 4, 7, 10]).astype(int)\n",
    "    \n",
    "    # Year-related features\n",
    "    min_year = df_temp[f'{prefix}year'].min()\n",
    "    df_temp[f'{prefix}years_since_start'] = df_temp[f'{prefix}year'] - min_year\n",
    "    \n",
    "    return df_temp\n",
    "\n",
    "def create_lag_features(df, date_col, target_col, lags=[1, 7, 30], prefix=''):\n",
    "    \"\"\"\n",
    "    Create lag features for time series analysis\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe (must be sorted by date)\n",
    "    date_col : str\n",
    "        Name of the date column\n",
    "    target_col : str\n",
    "        Name of the target column to create lags for\n",
    "    lags : list\n",
    "        List of lag periods\n",
    "    prefix : str\n",
    "        Prefix for new column names\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Dataframe with added lag features\n",
    "    \"\"\"\n",
    "    df_temp = df.copy()\n",
    "    \n",
    "    if target_col not in df_temp.columns:\n",
    "        return df_temp\n",
    "    \n",
    "    prefix = f\"{prefix}_\" if prefix else \"\"\n",
    "    \n",
    "    # Sort by date\n",
    "    df_temp = df_temp.sort_values(date_col)\n",
    "    \n",
    "    # Create lag features\n",
    "    for lag in lags:\n",
    "        df_temp[f'{prefix}{target_col}_lag_{lag}'] = df_temp[target_col].shift(lag)\n",
    "    \n",
    "    return df_temp\n",
    "\n",
    "def create_rolling_features(df, date_col, target_col, windows=[7, 30, 90], prefix=''):\n",
    "    \"\"\"\n",
    "    Create rolling window features\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe (must be sorted by date)\n",
    "    date_col : str\n",
    "        Name of the date column\n",
    "    target_col : str\n",
    "        Name of the target column to create rolling features for\n",
    "    windows : list\n",
    "        List of window sizes\n",
    "    prefix : str\n",
    "        Prefix for new column names\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Dataframe with added rolling features\n",
    "    \"\"\"\n",
    "    df_temp = df.copy()\n",
    "    \n",
    "    if target_col not in df_temp.columns:\n",
    "        return df_temp\n",
    "    \n",
    "    prefix = f\"{prefix}_\" if prefix else \"\"\n",
    "    \n",
    "    # Sort by date\n",
    "    df_temp = df_temp.sort_values(date_col)\n",
    "    \n",
    "    # Create rolling features\n",
    "    for window in windows:\n",
    "        df_temp[f'{prefix}{target_col}_rolling_mean_{window}'] = df_temp[target_col].rolling(window=window, min_periods=1).mean()\n",
    "        df_temp[f'{prefix}{target_col}_rolling_std_{window}'] = df_temp[target_col].rolling(window=window, min_periods=1).std()\n",
    "        df_temp[f'{prefix}{target_col}_rolling_max_{window}'] = df_temp[target_col].rolling(window=window, min_periods=1).max()\n",
    "        df_temp[f'{prefix}{target_col}_rolling_min_{window}'] = df_temp[target_col].rolling(window=window, min_periods=1).min()\n",
    "    \n",
    "    return df_temp\n",
    "\n",
    "print(\"✅ Temporal feature engineering functions defined\")\n",
    "print(\"   📅 extract_temporal_features - Basic & cyclical features\")\n",
    "print(\"   📊 create_lag_features - Historical lag features\")\n",
    "print(\"   🔄 create_rolling_features - Rolling window statistics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e51af800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏰ APPLYING TEMPORAL FEATURE ENGINEERING\n",
      "============================================================\n",
      "\n",
      "🔧 Processing: details_adventure_gear\n",
      "----------------------------------------\n",
      "   ❌ No date/time columns found - skipping temporal feature engineering\n",
      "\n",
      "   📊 Summary for details_adventure_gear:\n",
      "      Original columns: 24\n",
      "      Final columns: 24\n",
      "      Features added: 0\n",
      "\n",
      "🔧 Processing: details_magic_items\n",
      "----------------------------------------\n",
      "   ❌ No date/time columns found - skipping temporal feature engineering\n",
      "\n",
      "   📊 Summary for details_magic_items:\n",
      "      Original columns: 22\n",
      "      Final columns: 22\n",
      "      Features added: 0\n",
      "\n",
      "🔧 Processing: details_weapons\n",
      "----------------------------------------\n",
      "   ❌ No date/time columns found - skipping temporal feature engineering\n",
      "\n",
      "   📊 Summary for details_weapons:\n",
      "      Original columns: 27\n",
      "      Final columns: 27\n",
      "      Features added: 0\n",
      "\n",
      "🔧 Processing: details_armor\n",
      "----------------------------------------\n",
      "   ❌ No date/time columns found - skipping temporal feature engineering\n",
      "\n",
      "   📊 Summary for details_armor:\n",
      "      Original columns: 44\n",
      "      Final columns: 44\n",
      "      Features added: 0\n",
      "\n",
      "🔧 Processing: details_potions\n",
      "----------------------------------------\n",
      "   ❌ No date/time columns found - skipping temporal feature engineering\n",
      "\n",
      "   📊 Summary for details_potions:\n",
      "      Original columns: 17\n",
      "      Final columns: 17\n",
      "      Features added: 0\n",
      "\n",
      "🔧 Processing: details_poisons\n",
      "----------------------------------------\n",
      "   ❌ No date/time columns found - skipping temporal feature engineering\n",
      "\n",
      "   📊 Summary for details_poisons:\n",
      "      Original columns: 20\n",
      "      Final columns: 20\n",
      "      Features added: 0\n",
      "\n",
      "🔧 Processing: all_products\n",
      "----------------------------------------\n",
      "   ❌ No date/time columns found - skipping temporal feature engineering\n",
      "\n",
      "   📊 Summary for all_products:\n",
      "      Original columns: 18\n",
      "      Final columns: 18\n",
      "      Features added: 0\n",
      "\n",
      "🔧 Processing: customers\n",
      "----------------------------------------\n",
      "   ❌ No date/time columns found - skipping temporal feature engineering\n",
      "\n",
      "   📊 Summary for customers:\n",
      "      Original columns: 17\n",
      "      Final columns: 17\n",
      "      Features added: 0\n",
      "\n",
      "🔧 Processing: sales\n",
      "----------------------------------------\n",
      "   📅 Engineering features for: date\n",
      "   ⚠️  Could not convert date to datetime\n",
      "      ✅ Added 0 basic temporal features\n",
      "      📊 Adding time series features for sales quantity...\n",
      "      ✅ Added 15 time series features\n",
      "   📅 Engineering features for: date_freq\n",
      "      ✅ Added 43 basic temporal features\n",
      "      📊 Adding time series features for sales quantity...\n",
      "      📝 Sample temporal features: ['date_freq_year', 'date_freq_month', 'date_freq_day', 'date_freq_weekday', 'date_freq_hour']...\n",
      "   📅 Engineering features for: date_label\n",
      "      ✅ Added 71 basic temporal features\n",
      "      📊 Adding time series features for sales quantity...\n",
      "      📝 Sample temporal features: ['date_label_year', 'date_label_month', 'date_label_day', 'date_label_weekday', 'date_label_hour']...\n",
      "\n",
      "   📊 Summary for sales:\n",
      "      Original columns: 24\n",
      "      Final columns: 95\n",
      "      Features added: 71\n",
      "      Feature breakdown: Basic temporal: 0, Time series: 15, Basic temporal: 43, Basic temporal: 71\n",
      "\n",
      "🎉 TEMPORAL FEATURE ENGINEERING COMPLETED!\n",
      "✅ Processed 9 datasets\n",
      "📊 Added temporal features to 1 tables\n"
     ]
    }
   ],
   "source": [
    "# Apply temporal feature engineering\n",
    "print(\"⏰ APPLYING TEMPORAL FEATURE ENGINEERING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "enhanced_dataframes = {}\n",
    "temporal_features_log = {}\n",
    "\n",
    "for table_name, df in dataframes.items():\n",
    "    print(f\"\\n🔧 Processing: {table_name}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    df_enhanced = df.copy()\n",
    "    original_columns = len(df_enhanced.columns)\n",
    "    features_added = []\n",
    "    \n",
    "    # Check if this table has datetime columns\n",
    "    if table_name in datetime_columns:\n",
    "        for date_col in datetime_columns[table_name]:\n",
    "            print(f\"   📅 Engineering features for: {date_col}\")\n",
    "            \n",
    "            # Extract basic temporal features\n",
    "            df_enhanced = extract_temporal_features(df_enhanced, date_col, prefix=date_col)\n",
    "            \n",
    "            # Count new features added\n",
    "            new_columns = len(df_enhanced.columns)\n",
    "            basic_features_added = new_columns - original_columns\n",
    "            features_added.append(f\"Basic temporal: {basic_features_added}\")\n",
    "            \n",
    "            print(f\"      ✅ Added {basic_features_added} basic temporal features\")\n",
    "            \n",
    "            # For sales data, add advanced time series features\n",
    "            if table_name == 'sales' and 'quantity' in df_enhanced.columns:\n",
    "                print(f\"      📊 Adding time series features for sales quantity...\")\n",
    "                \n",
    "                original_ts_columns = len(df_enhanced.columns)\n",
    "                \n",
    "                # Create lag features (if we have enough data)\n",
    "                if len(df_enhanced) > 30:\n",
    "                    df_enhanced = create_lag_features(df_enhanced, date_col, 'quantity', \n",
    "                                                    lags=[1, 7, 30], prefix='sales')\n",
    "                \n",
    "                # Create rolling features (if we have enough data)\n",
    "                if len(df_enhanced) > 90:\n",
    "                    df_enhanced = create_rolling_features(df_enhanced, date_col, 'quantity', \n",
    "                                                        windows=[7, 30, 90], prefix='sales')\n",
    "                \n",
    "                ts_features_added = len(df_enhanced.columns) - original_ts_columns\n",
    "                if ts_features_added > 0:\n",
    "                    features_added.append(f\"Time series: {ts_features_added}\")\n",
    "                    print(f\"      ✅ Added {ts_features_added} time series features\")\n",
    "            \n",
    "            # Show some sample temporal features\n",
    "            temporal_cols = [col for col in df_enhanced.columns \n",
    "                           if col.startswith(f'{date_col}_') and col not in df.columns]\n",
    "            \n",
    "            if temporal_cols:\n",
    "                print(f\"      📝 Sample temporal features: {temporal_cols[:5]}...\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"   ❌ No date/time columns found - skipping temporal feature engineering\")\n",
    "    \n",
    "    # Final summary for this table\n",
    "    final_columns = len(df_enhanced.columns)\n",
    "    total_features_added = final_columns - original_columns\n",
    "    \n",
    "    print(f\"\\n   📊 Summary for {table_name}:\")\n",
    "    print(f\"      Original columns: {original_columns}\")\n",
    "    print(f\"      Final columns: {final_columns}\")\n",
    "    print(f\"      Features added: {total_features_added}\")\n",
    "    \n",
    "    if features_added:\n",
    "        print(f\"      Feature breakdown: {', '.join(features_added)}\")\n",
    "    \n",
    "    # Store enhanced dataframe\n",
    "    enhanced_dataframes[table_name] = df_enhanced\n",
    "    \n",
    "    # Log temporal features added\n",
    "    temporal_features_log[table_name] = {\n",
    "        'original_columns': original_columns,\n",
    "        'final_columns': final_columns,\n",
    "        'features_added': total_features_added,\n",
    "        'feature_breakdown': features_added,\n",
    "        'datetime_columns': datetime_columns.get(table_name, [])\n",
    "    }\n",
    "\n",
    "print(f\"\\n🎉 TEMPORAL FEATURE ENGINEERING COMPLETED!\")\n",
    "print(f\"✅ Processed {len(enhanced_dataframes)} datasets\")\n",
    "print(f\"📊 Added temporal features to {len([t for t in temporal_features_log.values() if t['features_added'] > 0])} tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9c6c939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 TEMPORAL FEATURE ENGINEERING SUMMARY:\n",
      "============================================================\n",
      "\n",
      "📊 details_adventure_gear:\n",
      "   Columns: 24 → 24 (+0)\n",
      "   No temporal features added\n",
      "\n",
      "📊 details_magic_items:\n",
      "   Columns: 22 → 22 (+0)\n",
      "   No temporal features added\n",
      "\n",
      "📊 details_weapons:\n",
      "   Columns: 27 → 27 (+0)\n",
      "   No temporal features added\n",
      "\n",
      "📊 details_armor:\n",
      "   Columns: 44 → 44 (+0)\n",
      "   No temporal features added\n",
      "\n",
      "📊 details_potions:\n",
      "   Columns: 17 → 17 (+0)\n",
      "   No temporal features added\n",
      "\n",
      "📊 details_poisons:\n",
      "   Columns: 20 → 20 (+0)\n",
      "   No temporal features added\n",
      "\n",
      "📊 all_products:\n",
      "   Columns: 18 → 18 (+0)\n",
      "   No temporal features added\n",
      "\n",
      "📊 customers:\n",
      "   Columns: 17 → 17 (+0)\n",
      "   No temporal features added\n",
      "\n",
      "📊 sales:\n",
      "   Columns: 24 → 95 (+71)\n",
      "   Date columns processed: date, date_freq, date_label\n",
      "   Feature types: Basic temporal: 0, Time series: 15, Basic temporal: 43, Basic temporal: 71\n",
      "\n",
      "🎯 OVERALL IMPACT:\n",
      "   Tables processed: 9\n",
      "   Tables with temporal features: 1\n",
      "   Original total columns: 213\n",
      "   Final total columns: 284\n",
      "   Total temporal features added: 71\n",
      "   Feature expansion ratio: 1.33x\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive temporal feature engineering summary\n",
    "print(\"📋 TEMPORAL FEATURE ENGINEERING SUMMARY:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total_original_columns = 0\n",
    "total_final_columns = 0\n",
    "total_features_added = 0\n",
    "tables_with_temporal = 0\n",
    "\n",
    "for table_name, log in temporal_features_log.items():\n",
    "    total_original_columns += log['original_columns']\n",
    "    total_final_columns += log['final_columns']\n",
    "    total_features_added += log['features_added']\n",
    "    \n",
    "    if log['features_added'] > 0:\n",
    "        tables_with_temporal += 1\n",
    "    \n",
    "    print(f\"\\n📊 {table_name}:\")\n",
    "    print(f\"   Columns: {log['original_columns']} → {log['final_columns']} (+{log['features_added']})\")\n",
    "    \n",
    "    if log['datetime_columns']:\n",
    "        print(f\"   Date columns processed: {', '.join(log['datetime_columns'])}\")\n",
    "    \n",
    "    if log['feature_breakdown']:\n",
    "        print(f\"   Feature types: {', '.join(log['feature_breakdown'])}\")\n",
    "    else:\n",
    "        print(f\"   No temporal features added\")\n",
    "\n",
    "print(f\"\\n🎯 OVERALL IMPACT:\")\n",
    "print(f\"   Tables processed: {len(temporal_features_log)}\")\n",
    "print(f\"   Tables with temporal features: {tables_with_temporal}\")\n",
    "print(f\"   Original total columns: {total_original_columns}\")\n",
    "print(f\"   Final total columns: {total_final_columns}\")\n",
    "print(f\"   Total temporal features added: {total_features_added}\")\n",
    "print(f\"   Feature expansion ratio: {total_final_columns/total_original_columns:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af3876a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 TEMPORAL PATTERN VISUALIZATION\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "DateParseError",
     "evalue": "Unknown datetime string format, unable to parse: other, at position 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDateParseError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m sales_df \u001b[38;5;241m=\u001b[39m enhanced_dataframes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msales\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Ensure date column is datetime\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m sales_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(sales_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Create visualization plots\u001b[39;00m\n\u001b[1;32m     12\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py:1063\u001b[0m, in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1061\u001b[0m             result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mtz_localize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, ABCSeries):\n\u001b[0;32m-> 1063\u001b[0m     cache_array \u001b[38;5;241m=\u001b[39m _maybe_cache(arg, \u001b[38;5;28mformat\u001b[39m, cache, convert_listlike)\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache_array\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m   1065\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py:247\u001b[0m, in \u001b[0;36m_maybe_cache\u001b[0;34m(arg, format, cache, convert_listlike)\u001b[0m\n\u001b[1;32m    245\u001b[0m unique_dates \u001b[38;5;241m=\u001b[39m unique(arg)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dates) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(arg):\n\u001b[0;32m--> 247\u001b[0m     cache_dates \u001b[38;5;241m=\u001b[39m convert_listlike(unique_dates, \u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# GH#45319\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/tools/datetimes.py:435\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _array_strptime_with_fallback(arg, name, utc, \u001b[38;5;28mformat\u001b[39m, exact, errors)\n\u001b[0;32m--> 435\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m objects_to_datetime64(\n\u001b[1;32m    436\u001b[0m     arg,\n\u001b[1;32m    437\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[1;32m    438\u001b[0m     yearfirst\u001b[38;5;241m=\u001b[39myearfirst,\n\u001b[1;32m    439\u001b[0m     utc\u001b[38;5;241m=\u001b[39mutc,\n\u001b[1;32m    440\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    441\u001b[0m     allow_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    442\u001b[0m )\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n\u001b[1;32m    447\u001b[0m     out_unit \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdatetime_data(result\u001b[38;5;241m.\u001b[39mdtype)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/arrays/datetimes.py:2398\u001b[0m, in \u001b[0;36mobjects_to_datetime64\u001b[0;34m(data, dayfirst, yearfirst, utc, errors, allow_object, out_unit)\u001b[0m\n\u001b[1;32m   2395\u001b[0m \u001b[38;5;66;03m# if str-dtype, convert\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(data, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mobject_)\n\u001b[0;32m-> 2398\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m tslib\u001b[38;5;241m.\u001b[39marray_to_datetime(\n\u001b[1;32m   2399\u001b[0m     data,\n\u001b[1;32m   2400\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m   2401\u001b[0m     utc\u001b[38;5;241m=\u001b[39mutc,\n\u001b[1;32m   2402\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[1;32m   2403\u001b[0m     yearfirst\u001b[38;5;241m=\u001b[39myearfirst,\n\u001b[1;32m   2404\u001b[0m     creso\u001b[38;5;241m=\u001b[39mabbrev_to_npy_unit(out_unit),\n\u001b[1;32m   2405\u001b[0m )\n\u001b[1;32m   2407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2408\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[1;32m   2409\u001b[0m     \u001b[38;5;66;03m#  is in UTC\u001b[39;00m\n\u001b[1;32m   2410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result, tz_parsed\n",
      "File \u001b[0;32mtslib.pyx:414\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mtslib.pyx:596\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mtslib.pyx:553\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mconversion.pyx:641\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.conversion.convert_str_to_tsobject\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsing.pyx:336\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing.parse_datetime_string\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsing.pyx:666\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing.dateutil_parse\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDateParseError\u001b[0m: Unknown datetime string format, unable to parse: other, at position 0"
     ]
    }
   ],
   "source": [
    "# Visualize temporal patterns (if sales data has date column)\n",
    "if 'sales' in enhanced_dataframes and 'date' in datetime_columns.get('sales', []):\n",
    "    print(\"📊 TEMPORAL PATTERN VISUALIZATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    sales_df = enhanced_dataframes['sales'].copy()\n",
    "    \n",
    "    # Ensure date column is datetime\n",
    "    sales_df['date'] = pd.to_datetime(sales_df['date'])\n",
    "    \n",
    "    # Create visualization plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Temporal Patterns in Sales Data', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Sales by month\n",
    "    if 'date_month' in sales_df.columns:\n",
    "        monthly_sales = sales_df.groupby('date_month')['quantity'].sum().reset_index()\n",
    "        axes[0, 0].bar(monthly_sales['date_month'], monthly_sales['quantity'])\n",
    "        axes[0, 0].set_title('Sales Volume by Month')\n",
    "        axes[0, 0].set_xlabel('Month')\n",
    "        axes[0, 0].set_ylabel('Total Quantity')\n",
    "        axes[0, 0].set_xticks(range(1, 13))\n",
    "    \n",
    "    # 2. Sales by day of week\n",
    "    if 'date_weekday' in sales_df.columns:\n",
    "        weekly_sales = sales_df.groupby('date_weekday')['quantity'].sum().reset_index()\n",
    "        day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "        axes[0, 1].bar(weekly_sales['date_weekday'], weekly_sales['quantity'])\n",
    "        axes[0, 1].set_title('Sales Volume by Day of Week')\n",
    "        axes[0, 1].set_xlabel('Day of Week')\n",
    "        axes[0, 1].set_ylabel('Total Quantity')\n",
    "        axes[0, 1].set_xticks(range(7))\n",
    "        axes[0, 1].set_xticklabels(day_names)\n",
    "    \n",
    "    # 3. Sales by quarter\n",
    "    if 'date_quarter' in sales_df.columns:\n",
    "        quarterly_sales = sales_df.groupby('date_quarter')['quantity'].sum().reset_index()\n",
    "        axes[1, 0].bar(quarterly_sales['date_quarter'], quarterly_sales['quantity'])\n",
    "        axes[1, 0].set_title('Sales Volume by Quarter')\n",
    "        axes[1, 0].set_xlabel('Quarter')\n",
    "        axes[1, 0].set_ylabel('Total Quantity')\n",
    "        axes[1, 0].set_xticks(range(1, 5))\n",
    "    \n",
    "    # 4. Sales time series (sample)\n",
    "    if len(sales_df) > 100:\n",
    "        # Sample for visualization\n",
    "        daily_sales = sales_df.groupby(sales_df['date'].dt.date)['quantity'].sum().reset_index()\n",
    "        sample_days = daily_sales.head(100)\n",
    "        axes[1, 1].plot(range(len(sample_days)), sample_days['quantity'])\n",
    "        axes[1, 1].set_title('Daily Sales Trend (First 100 Days)')\n",
    "        axes[1, 1].set_xlabel('Day')\n",
    "        axes[1, 1].set_ylabel('Daily Quantity')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✅ Temporal pattern visualizations completed\")\n",
    "else:\n",
    "    print(\"📊 Skipping visualization - no sales date data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7f93a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 TEMPORAL FEATURE VALIDATION\n",
      "==================================================\n",
      "\n",
      "📋 Validating: details_adventure_gear\n",
      "   📊 Rows: 106\n",
      "   📊 Columns: 24\n",
      "   ⏰ Temporal features: 0\n",
      "   ❓ Missing values in temporal features: 0\n",
      "   ✅ Data quality: Good\n",
      "\n",
      "📋 Validating: details_magic_items\n",
      "   📊 Rows: 199\n",
      "   📊 Columns: 22\n",
      "   ⏰ Temporal features: 0\n",
      "   ❓ Missing values in temporal features: 0\n",
      "   ✅ Data quality: Good\n",
      "\n",
      "📋 Validating: details_weapons\n",
      "   📊 Rows: 37\n",
      "   📊 Columns: 27\n",
      "   ⏰ Temporal features: 0\n",
      "   ❓ Missing values in temporal features: 0\n",
      "   ✅ Data quality: Good\n",
      "\n",
      "📋 Validating: details_armor\n",
      "   📊 Rows: 13\n",
      "   📊 Columns: 44\n",
      "   ⏰ Temporal features: 0\n",
      "   ❓ Missing values in temporal features: 0\n",
      "   ✅ Data quality: Good\n",
      "\n",
      "📋 Validating: details_potions\n",
      "   📊 Rows: 22\n",
      "   📊 Columns: 17\n",
      "   ⏰ Temporal features: 0\n",
      "   ❓ Missing values in temporal features: 0\n",
      "   ✅ Data quality: Good\n",
      "\n",
      "📋 Validating: details_poisons\n",
      "   📊 Rows: 16\n",
      "   📊 Columns: 20\n",
      "   ⏰ Temporal features: 0\n",
      "   ❓ Missing values in temporal features: 0\n",
      "   ✅ Data quality: Good\n",
      "\n",
      "📋 Validating: all_products\n",
      "   📊 Rows: 393\n",
      "   📊 Columns: 18\n",
      "   ⏰ Temporal features: 0\n",
      "   ❓ Missing values in temporal features: 0\n",
      "   ✅ Data quality: Good\n",
      "\n",
      "📋 Validating: customers\n",
      "   📊 Rows: 1,423\n",
      "   📊 Columns: 17\n",
      "   ⏰ Temporal features: 0\n",
      "   ❓ Missing values in temporal features: 0\n",
      "   ✅ Data quality: Good\n",
      "\n",
      "📋 Validating: sales\n",
      "   📊 Rows: 54,126\n",
      "   📊 Columns: 95\n",
      "   ⏰ Temporal features: 71\n",
      "   ❓ Missing values in temporal features: 41\n",
      "   ✅ Data quality: Needs attention\n",
      "\n",
      "🎯 VALIDATION SUMMARY:\n",
      "   Total temporal features created: 71\n",
      "   Total missing values in temporal features: 41\n",
      "   Tables with data quality issues: 1\n",
      "   ⚠️  Some issues found - review above details\n"
     ]
    }
   ],
   "source": [
    "# Validate temporal features\n",
    "print(\"🔍 TEMPORAL FEATURE VALIDATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "validation_results = {}\n",
    "\n",
    "for table_name, df in enhanced_dataframes.items():\n",
    "    print(f\"\\n📋 Validating: {table_name}\")\n",
    "    validation = {\n",
    "        'total_rows': len(df),\n",
    "        'total_columns': len(df.columns),\n",
    "        'temporal_features': 0,\n",
    "        'missing_values': 0,\n",
    "        'data_quality': 'Good'\n",
    "    }\n",
    "    \n",
    "    # Count temporal features\n",
    "    temporal_cols = [col for col in df.columns if any(pattern in col \n",
    "                    for pattern in ['_year', '_month', '_day', '_weekday', '_hour', \n",
    "                                  '_quarter', '_sin', '_cos', '_is_', '_lag_', '_rolling_'])]\n",
    "    validation['temporal_features'] = len(temporal_cols)\n",
    "    \n",
    "    # Check for missing values in temporal features\n",
    "    if temporal_cols:\n",
    "        missing_values = df[temporal_cols].isnull().sum().sum()\n",
    "        validation['missing_values'] = missing_values\n",
    "        \n",
    "        if missing_values > 0:\n",
    "            validation['data_quality'] = 'Needs attention'\n",
    "    \n",
    "    # Basic data quality checks for temporal features\n",
    "    quality_issues = []\n",
    "    \n",
    "    # Check month values (should be 1-12)\n",
    "    month_cols = [col for col in df.columns if '_month' in col and not ('_sin' in col or '_cos' in col)]\n",
    "    for col in month_cols:\n",
    "        if df[col].min() < 1 or df[col].max() > 12:\n",
    "            quality_issues.append(f\"Invalid month values in {col}\")\n",
    "    \n",
    "    # Check weekday values (should be 0-6)\n",
    "    weekday_cols = [col for col in df.columns if '_weekday' in col and not ('_sin' in col or '_cos' in col)]\n",
    "    for col in weekday_cols:\n",
    "        if df[col].min() < 0 or df[col].max() > 6:\n",
    "            quality_issues.append(f\"Invalid weekday values in {col}\")\n",
    "    \n",
    "    # Check cyclical features (should be between -1 and 1)\n",
    "    cyclical_cols = [col for col in df.columns if '_sin' in col or '_cos' in col]\n",
    "    for col in cyclical_cols:\n",
    "        if df[col].min() < -1.1 or df[col].max() > 1.1:  # Allow small floating point errors\n",
    "            quality_issues.append(f\"Invalid cyclical values in {col}\")\n",
    "    \n",
    "    if quality_issues:\n",
    "        validation['data_quality'] = 'Issues found'\n",
    "        validation['quality_issues'] = quality_issues\n",
    "    \n",
    "    validation_results[table_name] = validation\n",
    "    \n",
    "    # Print validation summary\n",
    "    print(f\"   📊 Rows: {validation['total_rows']:,}\")\n",
    "    print(f\"   📊 Columns: {validation['total_columns']}\")\n",
    "    print(f\"   ⏰ Temporal features: {validation['temporal_features']}\")\n",
    "    print(f\"   ❓ Missing values in temporal features: {validation['missing_values']}\")\n",
    "    print(f\"   ✅ Data quality: {validation['data_quality']}\")\n",
    "    \n",
    "    if 'quality_issues' in validation:\n",
    "        for issue in validation['quality_issues']:\n",
    "            print(f\"      ⚠️  {issue}\")\n",
    "\n",
    "print(f\"\\n🎯 VALIDATION SUMMARY:\")\n",
    "total_temporal_features = sum(v['temporal_features'] for v in validation_results.values())\n",
    "total_missing = sum(v['missing_values'] for v in validation_results.values())\n",
    "tables_with_issues = len([v for v in validation_results.values() if v['data_quality'] != 'Good'])\n",
    "\n",
    "print(f\"   Total temporal features created: {total_temporal_features}\")\n",
    "print(f\"   Total missing values in temporal features: {total_missing}\")\n",
    "print(f\"   Tables with data quality issues: {tables_with_issues}\")\n",
    "\n",
    "if total_missing == 0 and tables_with_issues == 0:\n",
    "    print(f\"   🎉 All temporal features passed validation!\")\n",
    "else:\n",
    "    print(f\"   ⚠️  Some issues found - review above details\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58cd5c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SAVING ENHANCED DATASETS\n",
      "========================================\n",
      "✅ Enhanced datasets saved to: data_intermediate/08_temporal_features_data.pkl\n",
      "✅ Temporal features log saved to: data_intermediate/08_temporal_features_log.pkl\n",
      "✅ Validation results saved to: data_intermediate/08_validation_results.pkl\n",
      "\n",
      "📊 FINAL DATASET SUMMARY:\n",
      "   details_adventure_gear: 106 rows × 24 columns\n",
      "   details_magic_items: 199 rows × 22 columns\n",
      "   details_weapons: 37 rows × 27 columns\n",
      "   details_armor: 13 rows × 44 columns\n",
      "   details_potions: 22 rows × 17 columns\n",
      "   details_poisons: 16 rows × 20 columns\n",
      "   all_products: 393 rows × 18 columns\n",
      "   customers: 1,423 rows × 17 columns\n",
      "   sales: 54,126 rows × 95 columns\n",
      "\n",
      "🎉 PHASE 8 COMPLETED SUCCESSFULLY!\n",
      "⏰ Temporal feature engineering finished\n",
      "📦 All datasets ready for machine learning with temporal features\n",
      "🚀 Ready for Phase 9: Feature Selection & Final Preparation\n"
     ]
    }
   ],
   "source": [
    "# Save enhanced datasets with temporal features\n",
    "print(\"💾 SAVING ENHANCED DATASETS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Save the enhanced dataframes\n",
    "try:\n",
    "    with open('data_intermediate/08_temporal_features_data.pkl', 'wb') as f:\n",
    "        pickle.dump(enhanced_dataframes, f)\n",
    "    \n",
    "    print(\"✅ Enhanced datasets saved to: data_intermediate/08_temporal_features_data.pkl\")\n",
    "    \n",
    "    # Save temporal features log\n",
    "    with open('data_intermediate/08_temporal_features_log.pkl', 'wb') as f:\n",
    "        pickle.dump(temporal_features_log, f)\n",
    "    \n",
    "    print(\"✅ Temporal features log saved to: data_intermediate/08_temporal_features_log.pkl\")\n",
    "    \n",
    "    # Save validation results\n",
    "    with open('data_intermediate/08_validation_results.pkl', 'wb') as f:\n",
    "        pickle.dump(validation_results, f)\n",
    "    \n",
    "    print(\"✅ Validation results saved to: data_intermediate/08_validation_results.pkl\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error saving files: {e}\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\n📊 FINAL DATASET SUMMARY:\")\n",
    "for table_name, df in enhanced_dataframes.items():\n",
    "    print(f\"   {table_name}: {len(df):,} rows × {len(df.columns)} columns\")\n",
    "\n",
    "print(f\"\\n🎉 PHASE 8 COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"⏰ Temporal feature engineering finished\")\n",
    "print(f\"📦 All datasets ready for machine learning with temporal features\")\n",
    "print(f\"🚀 Ready for Phase 9: Feature Selection & Final Preparation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d10d414",
   "metadata": {},
   "source": [
    "## 📋 **Temporal Feature Engineering Summary**\n",
    "\n",
    "### 🎯 **Phase 8 Achievements**\n",
    "\n",
    "This phase successfully engineered temporal features from date/time columns to enhance machine learning model performance:\n",
    "\n",
    "**✅ Temporal Features Created:**\n",
    "1. **Basic Components** - Year, month, day, weekday, hour, quarter, day of year, week of year\n",
    "2. **Cyclical Features** - Sine/cosine transformations for periodic patterns (month, weekday, hour, day of year)\n",
    "3. **Business Features** - Weekend flags, season indicators, quarter markers\n",
    "4. **Time Series Features** - Lag features and rolling window statistics (for sales data)\n",
    "5. **Year-based Features** - Years since data start for trend analysis\n",
    "\n",
    "**🔍 Key Insights:**\n",
    "- Date/time columns identified and processed across all relevant tables\n",
    "- Cyclical encoding preserves periodic patterns for ML algorithms\n",
    "- Business-relevant temporal features support domain-specific modeling\n",
    "- Time series features enable historical pattern recognition\n",
    "\n",
    "### 🚀 **Next Steps**\n",
    "\n",
    "1. **Feature Selection** - Identify most predictive temporal features\n",
    "2. **Feature Scaling** - Normalize temporal features for ML compatibility\n",
    "3. **Model Development** - Leverage temporal patterns in ML models\n",
    "4. **Seasonal Analysis** - Analyze business seasonality and trends\n",
    "\n",
    "**📝 Files Generated:**\n",
    "- `data_intermediate/08_temporal_features_data.pkl` - Enhanced datasets with temporal features\n",
    "- `data_intermediate/08_temporal_features_log.pkl` - Detailed feature engineering log\n",
    "- `data_intermediate/08_validation_results.pkl` - Quality validation results\n",
    "\n",
    "The datasets are now enriched with comprehensive temporal features and ready for advanced machine learning modeling!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
